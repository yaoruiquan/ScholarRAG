"""
RAGAS è¯„ä¼°æ¨¡å—
ä½¿ç”¨ RAGAS åº“è¯„ä¼° ScholarRAG ç³»ç»Ÿçš„æ£€ç´¢å’Œç”Ÿæˆè´¨é‡
é…ç½®ä¸ºä½¿ç”¨åƒé—® (Qwen) ä½œä¸ºè¯„ä¼° LLM
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

from dotenv import load_dotenv
load_dotenv()

from datasets import Dataset
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# é…ç½® Qwen LLM ç”¨äº RAGAS è¯„ä¼°
def get_eval_llm():
    """è·å–ç”¨äº RAGAS è¯„ä¼°çš„ LLM"""
    return ChatOpenAI(
        model="qwen-plus",
        api_key=os.getenv("QWEN_API_KEY"),
        base_url=os.getenv("QWEN_API_BASE", "https://dashscope.aliyuncs.com/compatible-mode/v1"),
        temperature=0
    )

def get_eval_embeddings():
    """è·å–ç”¨äº RAGAS è¯„ä¼°çš„ Embeddings"""
    # ä½¿ç”¨ Ollama embeddings
    from src.ingest import CustomOllamaEmbeddings
    return CustomOllamaEmbeddings()


class SimpleEvaluator:
    """
    ç®€åŒ–ç‰ˆ RAG è¯„ä¼°å™¨
    ä¸ä¾èµ– RAGAS çš„è‡ªåŠ¨è¯„ä¼°ï¼Œä½¿ç”¨ LLM-as-Judge æ–¹å¼
    """
    
    EVAL_PROMPT = """è¯·è¯„ä¼°ä»¥ä¸‹ RAG ç³»ç»Ÿçš„å›ç­”è´¨é‡ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‘
{context}

ã€ç³»ç»Ÿå›ç­”ã€‘
{answer}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰ï¼š
1. å¿ å®åº¦ (Faithfulness): å›ç­”æ˜¯å¦å®Œå…¨åŸºäºä¸Šä¸‹æ–‡ï¼Œæ²¡æœ‰ç¼–é€ ä¿¡æ¯
2. ç›¸å…³æ€§ (Relevance): å›ç­”æ˜¯å¦åˆ‡é¢˜ï¼Œç›´æ¥å›ç­”äº†ç”¨æˆ·é—®é¢˜
3. å®Œæ•´æ€§ (Completeness): å›ç­”æ˜¯å¦æ¶µç›–äº†ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®ä¿¡æ¯
4. è¿è´¯æ€§ (Coherence): å›ç­”æ˜¯å¦è¡¨è¾¾æ¸…æ™°ã€é€»è¾‘é€šé¡º

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
{{"faithfulness": X, "relevance": X, "completeness": X, "coherence": X, "overall": X, "comment": "ç®€çŸ­è¯„ä»·"}}
"""
    
    def __init__(self, rag_system=None):
        self.rag_system = rag_system
        self.llm = get_eval_llm()
    
    def evaluate_single(self, question: str, answer: str, contexts: List[str]) -> Dict:
        """è¯„ä¼°å•ä¸ªé—®ç­”å¯¹"""
        context_text = "\n\n---\n\n".join(contexts[:3])  # åªå–å‰3ä¸ªä¸Šä¸‹æ–‡
        
        prompt = self.EVAL_PROMPT.format(
            question=question,
            context=context_text[:2000],  # é™åˆ¶é•¿åº¦
            answer=answer
        )
        
        try:
            response = self.llm.invoke(prompt)
            # å°è¯•è§£æ JSON
            import re
            json_match = re.search(r'\{[^}]+\}', response.content)
            if json_match:
                return json.loads(json_match.group())
        except Exception as e:
            print(f"  è¯„ä¼°å¤±è´¥: {e}")
        
        return {"error": "è¯„ä¼°å¤±è´¥"}
    
    def run_rag_on_questions(self, questions: List[str]) -> List[Dict]:
        """å¯¹é—®é¢˜åˆ—è¡¨è¿è¡Œ RAG ç³»ç»Ÿ"""
        if self.rag_system is None:
            raise ValueError("RAG system not initialized")
        
        results = []
        for i, q in enumerate(questions):
            print(f"  [{i+1}/{len(questions)}] {q[:30]}...")
            answer, docs = self.rag_system.get_answer(q)
            contexts = [doc.page_content for doc in docs]
            results.append({
                "question": q,
                "answer": answer,
                "contexts": contexts
            })
        return results
    
    def evaluate_batch(self, questions: List[str]) -> Dict:
        """æ‰¹é‡è¯„ä¼°"""
        print(f"ğŸ“Š å¼€å§‹è¯„ä¼° {len(questions)} ä¸ªé—®é¢˜...")
        
        # è¿è¡Œ RAG
        rag_results = self.run_rag_on_questions(questions)
        
        # LLM è¯„ä¼°
        print("ğŸ” è¿è¡Œ LLM è¯„ä¼°...")
        scores = []
        for r in rag_results:
            score = self.evaluate_single(r["question"], r["answer"], r["contexts"])
            scores.append(score)
            r["scores"] = score
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_scores = {}
        for key in ["faithfulness", "relevance", "completeness", "coherence", "overall"]:
            valid_scores = [s.get(key, 0) for s in scores if isinstance(s.get(key), (int, float))]
            if valid_scores:
                avg_scores[key] = sum(valid_scores) / len(valid_scores)
        
        return {
            "timestamp": datetime.now().isoformat(),
            "num_samples": len(questions),
            "avg_scores": avg_scores,
            "details": rag_results
        }
    
    def generate_report(self, eval_result: Dict, output_path: str = None) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        avg = eval_result.get("avg_scores", {})
        
        report = f"""
==================== RAG è¯„ä¼°æŠ¥å‘Š ====================
è¯„æµ‹æ—¶é—´: {eval_result['timestamp']}
æµ‹è¯•æ ·æœ¬æ•°: {eval_result['num_samples']}

ã€å¹³å‡å¾—åˆ†ã€‘(æ»¡åˆ† 5 åˆ†)
  å¿ å®åº¦ (Faithfulness):  {avg.get('faithfulness', 'N/A'):.2f}
  ç›¸å…³æ€§ (Relevance):     {avg.get('relevance', 'N/A'):.2f}
  å®Œæ•´æ€§ (Completeness):  {avg.get('completeness', 'N/A'):.2f}
  è¿è´¯æ€§ (Coherence):     {avg.get('coherence', 'N/A'):.2f}
  ç»¼åˆå¾—åˆ† (Overall):     {avg.get('overall', 'N/A'):.2f}

========================================================
"""
        print(report)
        
        if output_path:
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(report)
                f.write("\n\nã€è¯¦ç»†ç»“æœã€‘\n")
                json.dump(eval_result["details"], f, ensure_ascii=False, indent=2)
            print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        return report


def quick_evaluate(rag_system, questions: List[str]) -> Dict:
    """
    å¿«é€Ÿè¯„ä¼°å‡½æ•°
    
    Args:
        rag_system: ScholarRAG å®ä¾‹
        questions: æµ‹è¯•é—®é¢˜åˆ—è¡¨
    
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    evaluator = SimpleEvaluator(rag_system)
    result = evaluator.evaluate_batch(questions)
    evaluator.generate_report(result)
    return result


def run_full_evaluation(kb_path: str = None, output_path: str = "./evaluation/reports/latest.txt"):
    """
    è¿è¡Œå®Œæ•´è¯„ä¼°
    
    Args:
        kb_path: çŸ¥è¯†åº“è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„
    """
    from src.rag_chain import ScholarRAG
    
    # åŠ è½½æµ‹è¯•é›†
    test_set_path = Path(__file__).parent / "data" / "test_set.json"
    with open(test_set_path, "r", encoding="utf-8") as f:
        test_set = json.load(f)
    
    questions = [item["question"] for item in test_set]
    
    # åˆå§‹åŒ– RAG
    if kb_path:
        rag = ScholarRAG(persist_directory=kb_path)
    else:
        rag = ScholarRAG()
    
    # è¯„ä¼°
    evaluator = SimpleEvaluator(rag)
    result = evaluator.evaluate_batch(questions)
    evaluator.generate_report(result, output_path)
    
    return result


if __name__ == "__main__":
    # æµ‹è¯•ç”¨ä¾‹
    from src.rag_chain import ScholarRAG
    
    # åˆå§‹åŒ– RAG
    rag = ScholarRAG(persist_directory="./data/kb_ee2fb36de231_db")
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "æ¯«ç±³æ³¢é›·è¾¾çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æ— äººæœºç›®æ ‡æ£€æµ‹æœ‰å“ªäº›ä¸»è¦æ–¹æ³•ï¼Ÿ",
    ]
    
    # å¿«é€Ÿè¯„ä¼°
    quick_evaluate(rag, test_questions)
