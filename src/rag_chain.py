"""
RAG é“¾æ¨¡å—
å®ç° BM25 + Vector æ··åˆæ£€ç´¢ä¸é—®ç­”
ä½¿ç”¨ Ollama æœ¬åœ° Embedding + åƒé—® LLM
"""

import os
from typing import List, Tuple, Optional

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ==================== é…ç½®å¸¸é‡ ====================

FAISS_PERSIST_DIR = "./data/faiss_db"
COLLECTION_NAME = "scholar_rag"

# æ£€ç´¢æƒé‡é…ç½®ï¼ˆæé«˜è¯­ä¹‰æ£€ç´¢æƒé‡ï¼‰
BM25_WEIGHT = 0.3
VECTOR_WEIGHT = 0.7

# æ£€ç´¢é…ç½®
INITIAL_TOP_K = 30    # åˆå§‹æ£€ç´¢æ•°é‡
RERANK_TOP_K = 15     # é‡æ’åä¿ç•™æ•°é‡ï¼ˆé€å…¥ LLMï¼‰
USE_RERANKER = True   # å¯ç”¨ Ollama Reranker


# ==================== Prompt Template ====================

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ç§‘ç ”è®ºæ–‡åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™å›ç­”é—®é¢˜ï¼š

ã€æ ¸å¿ƒè§„åˆ™ - å¿…é¡»éµå®ˆã€‘
1. **åªä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡**å›ç­”é—®é¢˜ï¼Œç¦æ­¢ä½¿ç”¨ä»»ä½•å¤–éƒ¨çŸ¥è¯†æˆ–å¸¸è¯†
2. å¦‚æœä¸Šä¸‹æ–‡ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œå¿…é¡»æ˜ç¡®å›ç­”ï¼š"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. æ¯ä¸ªå…³é”®è®ºç‚¹å¿…é¡»æ ‡æ³¨æ¥æºï¼š[æ–‡ä»¶å, Page X]
4. ç¦æ­¢ç¼–é€ ã€æ¨æµ‹æˆ–æ·»åŠ ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰çš„å†…å®¹

ã€å›ç­”æ ¼å¼ã€‘
1. é¦–å…ˆç»™å‡º 1-2 å¥ç®€æ´çš„ç»“è®º
2. ç„¶åè¯¦ç»†è§£é‡Šï¼Œæ¯æ®µæœ«å°¾æ ‡æ³¨å¼•ç”¨æ¥æº
3. ä½¿ç”¨ä¸“ä¸šçš„å­¦æœ¯è¯­è¨€

ã€ä¸Šä¸‹æ–‡å†…å®¹ã€‘
{context}
"""

# æ”¯æŒå¯¹è¯å†å²çš„ç”¨æˆ·æç¤º
USER_PROMPT_WITH_HISTORY = """ã€å¯¹è¯å†å²ã€‘
{chat_history}

ã€å½“å‰é—®é¢˜ã€‘{input}

è¯·ä¸¥æ ¼åŸºäºä¸Šä¸‹æ–‡å›ç­”ï¼Œæ ‡æ³¨æ¯ä¸ªè®ºç‚¹çš„å¼•ç”¨æ¥æº [æ–‡ä»¶å, Page X]ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥è¯´æ˜æ— æ³•å›ç­”ã€‚
æ³¨æ„ï¼šä½ å¯ä»¥å‚è€ƒå¯¹è¯å†å²æ¥ç†è§£ç”¨æˆ·é—®é¢˜çš„ä¸Šä¸‹æ–‡ï¼ˆå¦‚ä»£è¯æŒ‡ä»£ï¼‰ï¼Œä½†å›ç­”å¿…é¡»åŸºäºä¸Šä¸‹æ–‡å†…å®¹ã€‚"""

# æ— å¯¹è¯å†å²çš„ç”¨æˆ·æç¤º
USER_PROMPT = """ã€é—®é¢˜ã€‘{input}

è¯·ä¸¥æ ¼åŸºäºä¸Šä¸‹æ–‡å›ç­”ï¼Œæ ‡æ³¨æ¯ä¸ªè®ºç‚¹çš„å¼•ç”¨æ¥æº [æ–‡ä»¶å, Page X]ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥è¯´æ˜æ— æ³•å›ç­”ã€‚"""

RAG_PROMPT = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT),
    ("human", USER_PROMPT)
])

# å¸¦å†å²çš„ Prompt æ¨¡æ¿
RAG_PROMPT_WITH_HISTORY = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT),
    ("human", USER_PROMPT_WITH_HISTORY)
])


# ==================== æ ¸å¿ƒå‡½æ•° ====================

def get_embeddings():
    """
    è·å– Embedding æ¨¡å‹
    
    ä½¿ç”¨ bge-m3 å¤šè¯­è¨€æ¨¡å‹ï¼ˆä¸­è‹±æ–‡æ•ˆæœéƒ½å¾ˆå¥½ï¼‰
    """
    from src.ingest import CustomOllamaEmbeddings
    embeddings = CustomOllamaEmbeddings(
        model="bge-m3",
        base_url="http://localhost:11434"
    )
    return embeddings


def load_vectorstore(
    persist_directory: str = FAISS_PERSIST_DIR
) -> FAISS:
    """
    åŠ è½½å·²æŒä¹…åŒ–çš„ FAISS å‘é‡æ•°æ®åº“
    
    Args:
        persist_directory: æŒä¹…åŒ–ç›®å½•
        
    Returns:
        FAISS å‘é‡æ•°æ®åº“å®ä¾‹
    """
    embeddings = get_embeddings()
    
    vectorstore = FAISS.load_local(
        persist_directory,
        embeddings,
        allow_dangerous_deserialization=True
    )
    
    return vectorstore


def create_ensemble_retriever(
    vectorstore: FAISS,
    bm25_weight: float = BM25_WEIGHT,
    vector_weight: float = VECTOR_WEIGHT,
    top_k: int = INITIAL_TOP_K
) -> EnsembleRetriever:
    """
    åˆ›å»º BM25 + Vector æ··åˆæ£€ç´¢å™¨ (EnsembleRetriever)
    
    Args:
        vectorstore: Chroma å‘é‡æ•°æ®åº“
        bm25_weight: BM25 æ£€ç´¢å™¨æƒé‡
        vector_weight: å‘é‡æ£€ç´¢å™¨æƒé‡
        top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
        
    Returns:
        EnsembleRetriever æ··åˆæ£€ç´¢å™¨
    """
    print("  ğŸ”„ æ­£åœ¨è·å–æ–‡æ¡£ç”¨äº BM25...")
    
    # 1. ä» FAISS è·å–æ‰€æœ‰æ–‡æ¡£ç”¨äº BM25
    try:
        # FAISS ä½¿ç”¨ docstore å­˜å‚¨æ–‡æ¡£
        docstore = vectorstore.docstore
        documents = []
        
        for doc_id in vectorstore.index_to_docstore_id.values():
            doc = docstore.search(doc_id)
            if doc and hasattr(doc, 'page_content'):
                documents.append(doc)
        
        print(f"  ğŸ“„ è·å–åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
        
        if not documents:
            raise ValueError("çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
        
        print(f"  ğŸ“š åˆ›å»º BM25 æ£€ç´¢å™¨ ({len(documents)} ä¸ªæ–‡æ¡£)...")
        
        # 2. åˆ›å»º BM25 æ£€ç´¢å™¨
        bm25_retriever = BM25Retriever.from_documents(documents)
        bm25_retriever.k = top_k
        
    except Exception as e:
        print(f"  âš ï¸ BM25 åˆ›å»ºå¤±è´¥: {e}ï¼Œä½¿ç”¨çº¯å‘é‡æ£€ç´¢")
        # å¦‚æœ BM25 å¤±è´¥ï¼Œåªä½¿ç”¨å‘é‡æ£€ç´¢
        vector_retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": top_k}
        )
        return vector_retriever  # type: ignore
    
    print("  ğŸ” åˆ›å»ºå‘é‡æ£€ç´¢å™¨...")
    
    # 3. åˆ›å»ºå‘é‡æ£€ç´¢å™¨
    vector_retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": top_k}
    )
    
    print("  ğŸ”— åˆ›å»ºæ··åˆæ£€ç´¢å™¨...")
    
    # 4. åˆ›å»ºæ··åˆæ£€ç´¢å™¨
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[bm25_weight, vector_weight]
    )
    
    return ensemble_retriever


class RerankingRetriever:
    """
    å¸¦ Reranker çš„æ£€ç´¢å™¨åŒ…è£…ç±»
    å®ç° LangChain Retriever æ¥å£
    """
    
    def __init__(self, base_retriever: EnsembleRetriever, rerank_top_k: int = RERANK_TOP_K):
        self.base_retriever = base_retriever
        self.rerank_top_k = rerank_top_k
        self._reranker = None
    
    @property
    def reranker(self):
        """å»¶è¿ŸåŠ è½½ Reranker"""
        if self._reranker is None and USE_RERANKER:
            from src.reranker import get_reranker
            self._reranker = get_reranker()
        return self._reranker
    
    def invoke(self, query: str, config=None) -> List[Document]:
        """æ£€ç´¢å¹¶é‡æ’åº"""
        # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
        if isinstance(query, dict):
            query = query.get("input", str(query))
        
        # 1. åˆå§‹æ£€ç´¢
        docs = self.base_retriever.invoke(query)
        
        # 2. Reranker é‡æ’åº
        if self.reranker and docs:
            docs = self.reranker.rerank(query, docs, self.rerank_top_k)
        
        return docs
    
    def get_relevant_documents(self, query: str) -> List[Document]:
        """å…¼å®¹æ—§ç‰ˆ LangChain æ¥å£"""
        return self.invoke(query)
    
    def with_config(self, **kwargs):
        """å…¼å®¹ LangChain Runnable æ¥å£"""
        return self


def create_reranking_retriever(vectorstore: FAISS):
    """
    åˆ›å»ºå¸¦ Query Expansion å’Œ Reranker çš„æ£€ç´¢å™¨
    
    Args:
        vectorstore: FAISS å‘é‡æ•°æ®åº“
        
    Returns:
        å…¼å®¹ LangChain çš„æ£€ç´¢å™¨
    """
    from langchain_core.runnables import RunnableLambda
    from src.query_expansion import expand_query
    
    # åˆ›å»ºåŸºç¡€æ··åˆæ£€ç´¢å™¨
    base_retriever = create_ensemble_retriever(vectorstore)
    
    # åˆ›å»º Reranker åŒ…è£…å™¨
    reranker_wrapper = RerankingRetriever(base_retriever)
    
    # ä½¿ç”¨ RunnableLambda åŒ…è£…ä»¥å…¼å®¹ LangChain
    def retrieve_with_expansion_and_rerank(query):
        if isinstance(query, dict):
            query = query.get("input", str(query))
        
        # 1. Query Expansion - ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“
        expanded_queries = expand_query(query, num_variants=3)
        
        # 2. å¯¹æ¯ä¸ªæŸ¥è¯¢å˜ä½“è¿›è¡Œæ£€ç´¢
        all_docs = []
        seen_contents = set()
        
        for q in expanded_queries:
            docs = base_retriever.invoke(q)
            for doc in docs:
                # å»é‡ï¼ˆåŸºäºå†…å®¹ï¼‰
                content_hash = hash(doc.page_content[:200])
                if content_hash not in seen_contents:
                    seen_contents.add(content_hash)
                    all_docs.append(doc)
        
        print(f"  ğŸ“š Multi-Query æ£€ç´¢: {len(expanded_queries)} ä¸ªæŸ¥è¯¢ â†’ {len(all_docs)} ä¸ªæ–‡æ¡£ï¼ˆå»é‡åï¼‰")
        
        # 3. Reranker é‡æ’åº
        if USE_RERANKER and reranker_wrapper.reranker and all_docs:
            all_docs = reranker_wrapper.reranker.rerank(query, all_docs, RERANK_TOP_K)
        
        return all_docs
    
    return RunnableLambda(retrieve_with_expansion_and_rerank)


def get_llm() -> ChatOpenAI:
    """
    åˆå§‹åŒ–åƒé—® (Qwen) LLM
    
    é€šè¿‡ OpenAI å…¼å®¹æ¥å£è¿æ¥é˜¿é‡Œäº‘ DashScope API
    
    Returns:
        ChatOpenAI å®ä¾‹
    """
    from pydantic import SecretStr
    
    api_key = os.getenv("QWEN_API_KEY")
    if not api_key:
        raise ValueError("è¯·è®¾ç½® QWEN_API_KEY ç¯å¢ƒå˜é‡")
    
    api_base = os.getenv("QWEN_API_BASE", "https://dashscope.aliyuncs.com/compatible-mode/v1")
    model_name = os.getenv("MODEL_NAME", "qwen-plus")
    
    llm = ChatOpenAI(
        model=model_name,
        api_key=SecretStr(api_key),
        base_url=api_base,
        temperature=0.3,
    )
    
    return llm


def build_rag_chain(retriever, use_history: bool = False):
    """
    æ„å»º RAG Chain
    
    Args:
        retriever: æ£€ç´¢å™¨
        use_history: æ˜¯å¦ä½¿ç”¨å¯¹è¯å†å²
        
    Returns:
        RAG Chain
    """
    llm = get_llm()
    
    # é€‰æ‹© Prompt æ¨¡æ¿
    prompt = RAG_PROMPT_WITH_HISTORY if use_history else RAG_PROMPT
    
    # åˆ›å»ºæ–‡æ¡£å¤„ç†é“¾
    question_answer_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt
    )
    
    # åˆ›å»ºå®Œæ•´ RAG é“¾
    rag_chain = create_retrieval_chain(
        retriever=retriever,
        combine_docs_chain=question_answer_chain
    )
    
    return rag_chain


# ==================== å¯¹å¤–æ¥å£ ====================

class ScholarRAG:
    """ScholarRAG é—®ç­”ç³»ç»Ÿå°è£…ç±»"""
    
    def __init__(
        self,
        persist_directory: str = FAISS_PERSIST_DIR
    ):
        """
        åˆå§‹åŒ– ScholarRAG ç³»ç»Ÿ
        
        Args:
            persist_directory: FAISS æŒä¹…åŒ–ç›®å½•
        """
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ– ScholarRAG ç³»ç»Ÿ...")
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        self.vectorstore = load_vectorstore(persist_directory)
        print("  âœ… å‘é‡æ•°æ®åº“åŠ è½½å®Œæˆ")
        
        # åˆ›å»ºå¸¦ Reranker çš„æ··åˆæ£€ç´¢å™¨
        self.retriever = create_reranking_retriever(self.vectorstore)
        if USE_RERANKER:
            print("  âœ… æ··åˆæ£€ç´¢å™¨åˆ›å»ºå®Œæˆ (BM25 + Vector + Reranker)")
        else:
            print("  âœ… æ··åˆæ£€ç´¢å™¨åˆ›å»ºå®Œæˆ (BM25 + Vector)")
        
        # æ„å»º RAG Chain
        self.rag_chain = build_rag_chain(self.retriever)
        print("  âœ… RAG Chain æ„å»ºå®Œæˆ")
        
        print("ğŸ‰ ScholarRAG åˆå§‹åŒ–å®Œæˆï¼")
    
    def get_answer(self, question: str, chat_history: Optional[List[dict]] = None) -> Tuple[str, List[Document]]:
        """
        è·å–é—®é¢˜çš„ç­”æ¡ˆï¼ˆæ”¯æŒå¯¹è¯å†å²ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            chat_history: å¯¹è¯å†å²åˆ—è¡¨ [{"role": "user/assistant", "content": "..."}]
            
        Returns:
            (answer, source_documents) å…ƒç»„
            - answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            - source_documents: å¼•ç”¨çš„æºæ–‡æ¡£åˆ—è¡¨
        """
        print(f"\nğŸ“ ç”¨æˆ·é—®é¢˜: {question}")
        
        # æ„å»ºè¾“å…¥
        invoke_input = {"input": question}
        
        # å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œæ ¼å¼åŒ–å¹¶æ·»åŠ 
        if chat_history and len(chat_history) > 0:
            # åªä¿ç•™æœ€è¿‘ 5 è½®å¯¹è¯
            recent_history = chat_history[-10:]  # æœ€å¤š 10 æ¡æ¶ˆæ¯ï¼ˆ5è½®ï¼‰
            
            # æ ¼å¼åŒ–å¯¹è¯å†å²
            history_text = ""
            for msg in recent_history:
                role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
                content = msg.get("content", "")[:200]  # æˆªæ–­è¿‡é•¿å†…å®¹
                history_text += f"{role}: {content}\n"
            
            invoke_input["chat_history"] = history_text.strip()
            print(f"ğŸ’­ ä½¿ç”¨å¯¹è¯å†å²: {len(recent_history)} æ¡æ¶ˆæ¯")
            
            # ä½¿ç”¨å¸¦å†å²çš„ RAG Chain
            rag_chain = build_rag_chain(self.retriever, use_history=True)
            result = rag_chain.invoke(invoke_input)
        else:
            # æ— å†å²ï¼Œä½¿ç”¨æ™®é€š RAG Chain
            result = self.rag_chain.invoke(invoke_input)
        
        answer = result.get("answer", "")
        source_docs = result.get("context", [])
        
        print(f"ğŸ“š æ£€ç´¢åˆ° {len(source_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
        if source_docs:
            for i, doc in enumerate(source_docs[:3]):
                print(f"  [{i+1}] {doc.page_content[:100]}...")
        print(f"ğŸ’¬ å›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")
        
        return answer, source_docs
    
    def format_sources(self, source_docs: List[Document]) -> str:
        """
        æ ¼å¼åŒ–æºæ–‡æ¡£ä¿¡æ¯
        
        Args:
            source_docs: æºæ–‡æ¡£åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–åçš„å¼•ç”¨ä¿¡æ¯å­—ç¬¦ä¸²
        """
        if not source_docs:
            return "æ— å¼•ç”¨æ–‡æ¡£"
        
        sources = []
        seen = set()
        
        for doc in source_docs:
            metadata = doc.metadata
            source = metadata.get("source", "æœªçŸ¥æ¥æº")
            page = metadata.get("page", "?")
            
            # æå–æ–‡ä»¶å
            if "/" in source or "\\" in source:
                source = source.replace("\\", "/").split("/")[-1]
            
            key = f"{source}_p{page}"
            if key not in seen:
                seen.add(key)
                sources.append(f"- {source} (Page {page})")
        
        return "\n".join(sources)


def get_answer(question: str) -> Tuple[str, List[Document]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–é—®é¢˜çš„ç­”æ¡ˆ
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        
    Returns:
        (answer, source_documents) å…ƒç»„
    """
    rag = ScholarRAG()
    return rag.get_answer(question)


# ==================== æµ‹è¯•å…¥å£ ====================

if __name__ == "__main__":
    # æµ‹è¯•é—®ç­”
    rag = ScholarRAG()
    
    test_question = "è¯·æ€»ç»“è¿™ç¯‡è®ºæ–‡çš„ä¸»è¦è´¡çŒ®"
    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {test_question}")
    print("-" * 50)
    
    answer, sources = rag.get_answer(test_question)
    
    print(f"ğŸ’¡ å›ç­”:\n{answer}")
    print("-" * 50)
    print(f"ğŸ“š å¼•ç”¨æ¥æº:\n{rag.format_sources(sources)}")
