"""
æ•°æ®å¤„ç†æ¨¡å— - PDF æ–‡æ¡£åŠ è½½ã€æ¸…æ´—ã€åˆ‡åˆ†ä¸å‘é‡åŒ–å­˜å‚¨
ä½¿ç”¨ Ollama æœ¬åœ° Embedding æ¨¡å‹ï¼ˆnomic-embed-textï¼‰
"""

import os
import re
from pathlib import Path
from typing import List

import fitz  # PyMuPDF
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from dotenv import load_dotenv

load_dotenv()


def load_pdfs(data_dir: str = "./data/pdfs") -> List[Document]:
    """
    åŠ è½½æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰ PDF æ–‡ä»¶ï¼ˆç›´æ¥ä½¿ç”¨ PyMuPDFï¼‰
    
    Args:
        data_dir: PDF æ–‡ä»¶æ‰€åœ¨ç›®å½•
        
    Returns:
        Document åˆ—è¡¨
    """
    documents = []
    pdf_dir = Path(data_dir)
    
    if not pdf_dir.exists():
        print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return documents
    
    pdf_files = list(pdf_dir.glob("*.pdf"))
    
    if not pdf_files:
        print(f"âš ï¸ ç›®å½• {data_dir} ä¸­æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶")
        return documents
    
    print(f"ğŸ“„ æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶")
    
    for pdf_path in pdf_files:
        try:
            print(f"  â³ æ­£åœ¨åŠ è½½: {pdf_path.name}")
            
            # ä½¿ç”¨ pdfplumber æå–è¡¨æ ¼
            import pdfplumber
            tables_text = []
            try:
                with pdfplumber.open(str(pdf_path)) as pdf:
                    for page_num, page in enumerate(pdf.pages):
                        # æå–è¡¨æ ¼
                        tables = page.extract_tables()
                        for table in tables:
                            if table:
                                # å°†è¡¨æ ¼è½¬æ¢ä¸ºç»“æ„åŒ–æ–‡æœ¬
                                table_str = format_table_as_text(table, page_num + 1)
                                if table_str:
                                    tables_text.append((page_num + 1, table_str))
            except Exception as e:
                print(f"    âš ï¸ è¡¨æ ¼æå–å¤±è´¥: {e}")
            
            # ä½¿ç”¨ PyMuPDF æå–æ™®é€šæ–‡æœ¬
            doc = fitz.open(str(pdf_path))
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text()
                
                if text and str(text).strip():
                    documents.append(Document(
                        page_content=str(text),
                        metadata={
                            "source": str(pdf_path),
                            "page": page_num + 1
                        }
                    ))
            
            # æ·»åŠ æå–çš„è¡¨æ ¼ä½œä¸ºå•ç‹¬çš„æ–‡æ¡£
            for page_num, table_text in tables_text:
                documents.append(Document(
                    page_content=table_text,
                    metadata={
                        "source": str(pdf_path),
                        "page": page_num,
                        "type": "table"
                    }
                ))
            
            page_count = len(doc)
            doc.close()
            print(f"  âœ… åŠ è½½å®Œæˆ: {pdf_path.name} ({page_count} é¡µ, {len(tables_text)} ä¸ªè¡¨æ ¼)")
        except Exception as e:
            print(f"  âŒ åŠ è½½å¤±è´¥: {pdf_path.name} - {e}")
    
    return documents


def format_table_as_text(table: list, page_num: int) -> str:
    """
    å°†è¡¨æ ¼è½¬æ¢ä¸ºç»“æ„åŒ–çš„è‡ªç„¶è¯­è¨€æè¿°
    """
    if not table or len(table) < 2:
        return ""
    
    # è·å–è¡¨å¤´
    headers = table[0]
    if not headers:
        return ""
    
    # æ¸…ç†ç©ºå€¼
    headers = [str(h).strip() if h else f"åˆ—{i+1}" for i, h in enumerate(headers)]
    
    rows_text = []
    for row in table[1:]:
        if not row:
            continue
        # å°†æ¯è¡Œè½¬æ¢ä¸º "å­—æ®µ: å€¼" æ ¼å¼
        row_parts = []
        for i, cell in enumerate(row):
            if cell and str(cell).strip():
                header = headers[i] if i < len(headers) else f"åˆ—{i+1}"
                row_parts.append(f"{header}: {str(cell).strip()}")
        if row_parts:
            rows_text.append("ï¼›".join(row_parts))
    
    if not rows_text:
        return ""
    
    return f"[è¡¨æ ¼å†…å®¹ - ç¬¬{page_num}é¡µ]\n" + "\n".join(rows_text)


def clean_text(text: str) -> str:
    """
    æ–‡æœ¬æ¸…æ´—é¢„å¤„ç†
    
    1. ä¿®å¤æ¢è¡Œç¬¦é€ æˆçš„å•è¯æ–­è£‚ (hyphenation fix)
    2. å»é™¤ References ä¹‹åçš„å†…å®¹
    3. è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        æ¸…æ´—åçš„æ–‡æœ¬
    """
    # 1. ä¿®å¤è¿å­—ç¬¦æ–­è¯ (hyphenation fix)
    # ä¾‹å¦‚: "knowl-\nedge" -> "knowledge"
    text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
    
    # 2. å»é™¤ References/Bibliography ä¹‹åçš„å†…å®¹
    # åŒ¹é…å¤šç§å‚è€ƒæ–‡çŒ®æ ‡é¢˜æ ¼å¼
    references_pattern = r'\n\s*(References|REFERENCES|Bibliography|BIBLIOGRAPHY|å‚è€ƒæ–‡çŒ®)\s*\n'
    match = re.search(references_pattern, text)
    if match:
        text = text[:match.start()]
    
    # 3. å°†å¤šä¸ªè¿ç»­æ¢è¡Œç¬¦æ›¿æ¢ä¸ºå•ä¸ªæ¢è¡Œ
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # 4. å»é™¤è¡Œé¦–è¡Œå°¾å¤šä½™ç©ºç™½
    text = '\n'.join(line.strip() for line in text.split('\n'))
    
    # 5. å»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r' {2,}', ' ', text)
    
    return text.strip()


def preprocess_documents(documents: List[Document]) -> List[Document]:
    """
    å¯¹æ–‡æ¡£åˆ—è¡¨è¿›è¡Œé¢„å¤„ç†
    
    Args:
        documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
        
    Returns:
        æ¸…æ´—åçš„æ–‡æ¡£åˆ—è¡¨
    """
    cleaned_docs = []
    
    for doc in documents:
        cleaned_content = clean_text(doc.page_content)
        if cleaned_content:  # åªä¿ç•™éç©ºæ–‡æ¡£
            cleaned_doc = Document(
                page_content=cleaned_content,
                metadata=doc.metadata
            )
            cleaned_docs.append(cleaned_doc)
    
    print(f"ğŸ§¹ æ–‡æ¡£æ¸…æ´—å®Œæˆ: {len(documents)} -> {len(cleaned_docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
    return cleaned_docs


def split_documents(
    documents: List[Document],
    chunk_size: int = 1000,
    chunk_overlap: int = 200
) -> List[Document]:
    """
    ä½¿ç”¨ RecursiveCharacterTextSplitter åˆ‡åˆ†æ–‡æ¡£
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        chunk_size: åˆ‡ç‰‡å¤§å°
        chunk_overlap: åˆ‡ç‰‡é‡å 
        
    Returns:
        åˆ‡åˆ†åçš„æ–‡æ¡£åˆ—è¡¨
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
    )
    
    chunks = text_splitter.split_documents(documents)
    print(f"âœ‚ï¸ æ–‡æ¡£åˆ‡åˆ†å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£ -> {len(chunks)} ä¸ª chunks")
    
    return chunks


class CustomOllamaEmbeddings(Embeddings):
    """
    è‡ªå®šä¹‰ Ollama Embeddings ç±»
    ç»§æ‰¿ LangChain Embeddings æ¥å£ï¼Œç›´æ¥ä½¿ç”¨ httpx è°ƒç”¨ API
    """
    
    def __init__(self, model: str = "nomic-embed-text", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url.rstrip("/")
        self.embed_url = f"{self.base_url}/api/embeddings"
        
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents"""
        import httpx
        embeddings = []
        
        with httpx.Client(timeout=120.0) as client:
            for idx, text in enumerate(texts):
                try:
                    response = client.post(
                        self.embed_url,
                        json={"model": self.model, "prompt": text}
                    )
                    response.raise_for_status()
                    data = response.json()
                    embeddings.append(data["embedding"])
                except Exception as e:
                    print(f"    âŒ Embedding #{idx} failed: {e}")
                    raise
        
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query"""
        return self.embed_documents([text])[0]


def get_embeddings():
    """
    è·å– Embedding æ¨¡å‹
    
    ä½¿ç”¨ bge-m3 å¤šè¯­è¨€æ¨¡å‹ï¼ˆä¸­è‹±æ–‡æ•ˆæœéƒ½å¾ˆå¥½ï¼‰
    
    Returns:
        Embeddings å®ä¾‹
    """
    print("ğŸ”„ æ­£åœ¨è¿æ¥ Ollama Embedding æ¨¡å‹ (bge-m3)...")
    
    embeddings = CustomOllamaEmbeddings(
        model="bge-m3",
        base_url="http://localhost:11434"
    )
    
    # æµ‹è¯•è¿æ¥
    try:
        test_result = embeddings.embed_query("test")
        print(f"âœ… Ollama Embedding è¿æ¥æˆåŠŸ (å‘é‡ç»´åº¦: {len(test_result)})")
    except Exception as e:
        print(f"âš ï¸ Ollama è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
    
    return embeddings


def create_vectorstore(
    chunks: List[Document],
    persist_directory: str = "./data/faiss_db",
    batch_size: int = 20,
    progress_callback=None
) -> "FAISS | None":
    """
    åˆ›å»ºå¹¶æŒä¹…åŒ– FAISS å‘é‡æ•°æ®åº“ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
    
    Args:
        chunks: æ–‡æ¡£åˆ‡ç‰‡åˆ—è¡¨
        persist_directory: æŒä¹…åŒ–å­˜å‚¨ç›®å½•
        batch_size: æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total, message)
        
    Returns:
        Chroma å‘é‡æ•°æ®åº“å®ä¾‹
    """
    import shutil
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆæ¸…ç†æ—§æ•°æ®ï¼‰
    if os.path.exists(persist_directory):
        shutil.rmtree(persist_directory)
    os.makedirs(persist_directory, exist_ok=True)
    
    # è·å– Embedding æ¨¡å‹
    embeddings = get_embeddings()
    
    total_chunks = len(chunks)
    print(f"ğŸ’¾ æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“ï¼Œå…± {total_chunks} ä¸ª chunksï¼ˆæ¯æ‰¹ {batch_size} ä¸ªï¼‰...")
    
    if progress_callback:
        progress_callback(0, total_chunks, "å¼€å§‹å¤„ç†...")
    
    vectorstore = None
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, total_chunks, batch_size):
        batch = chunks[i:i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (total_chunks + batch_size - 1) // batch_size
        
        print(f"  ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ä¸ªæ–‡æ¡£)...")
        
        if progress_callback:
            progress_callback(i, total_chunks, f"æ­£åœ¨ Embedding ç¬¬ {batch_num}/{total_batches} æ‰¹...")
        
        if vectorstore is None:
            # ç¬¬ä¸€æ‰¹ï¼šåˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
            vectorstore = FAISS.from_documents(
                documents=batch,
                embedding=embeddings
            )
        else:
            # åç»­æ‰¹æ¬¡ï¼šæ·»åŠ åˆ°ç°æœ‰æ•°æ®åº“
            vectorstore.add_documents(batch)
        
        print(f"  âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ")
    
    if progress_callback:
        progress_callback(total_chunks, total_chunks, "å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼")
    
    # ä¿å­˜ FAISS ç´¢å¼•
    if vectorstore:
        # è½¬ä¸ºç»å¯¹è·¯å¾„å¹¶ç¡®ä¿ç›®å½•å­˜åœ¨
        abs_path = Path(persist_directory).resolve()
        abs_path.mkdir(parents=True, exist_ok=True)
        print(f"  ğŸ’¾ ä¿å­˜åˆ°: {abs_path}")
        vectorstore.save_local(str(abs_path))
    
    print(f"âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼Œå·²æŒä¹…åŒ–åˆ°: {persist_directory}")
    
    return vectorstore


def load_existing_vectorstore(persist_directory: str = "./data/faiss_db") -> FAISS:
    """
    åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“
    
    Args:
        persist_directory: æŒä¹…åŒ–å­˜å‚¨ç›®å½•
        
    Returns:
        FAISS å‘é‡æ•°æ®åº“å®ä¾‹
    """
    embeddings = get_embeddings()
    
    vectorstore = FAISS.load_local(
        persist_directory,
        embeddings,
        allow_dangerous_deserialization=True
    )
    
    print(f"âœ… å·²åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“: {persist_directory}")
    return vectorstore


def ingest_pdfs(
    data_dir: str = "./data/pdfs",
    persist_directory: str = "./data/faiss_db",
    chunk_size: int = 1000,
    chunk_overlap: int = 200
) -> "FAISS | None":
    """
    å®Œæ•´çš„ PDF æ•°æ®æ‘„å…¥æµç¨‹
    
    1. åŠ è½½ PDF æ–‡ä»¶
    2. æ¸…æ´—æ–‡æœ¬
    3. åˆ‡åˆ†æ–‡æ¡£
    4. åˆ›å»ºå‘é‡æ•°æ®åº“
    
    Args:
        data_dir: PDF æ–‡ä»¶ç›®å½•
        persist_directory: å‘é‡æ•°æ®åº“å­˜å‚¨ç›®å½•
        chunk_size: åˆ‡ç‰‡å¤§å°
        chunk_overlap: åˆ‡ç‰‡é‡å 
        
    Returns:
        Chroma å‘é‡æ•°æ®åº“å®ä¾‹
    """
    print("=" * 50)
    print("ğŸš€ å¼€å§‹ PDF æ•°æ®æ‘„å…¥æµç¨‹")
    print("=" * 50)
    
    # Step 1: åŠ è½½ PDF
    documents = load_pdfs(data_dir)
    if not documents:
        raise ValueError("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ PDF æ–‡ä»¶è·¯å¾„")
    
    # Step 2: æ¸…æ´—æ–‡æœ¬
    cleaned_docs = preprocess_documents(documents)
    
    # Step 3: åˆ‡åˆ†æ–‡æ¡£
    chunks = split_documents(cleaned_docs, chunk_size, chunk_overlap)
    
    # Step 4: åˆ›å»ºå‘é‡æ•°æ®åº“
    vectorstore = create_vectorstore(chunks, persist_directory)
    
    print("=" * 50)
    print(f"ğŸ‰ æ•°æ®æ‘„å…¥å®Œæˆï¼")
    print(f"   ğŸ“„ å¤„ç†æ–‡æ¡£: {len(documents)} ä¸ª")
    print(f"   âœ‚ï¸ ç”Ÿæˆåˆ‡ç‰‡: {len(chunks)} ä¸ª chunks")
    print(f"   ğŸ’¾ å­˜å‚¨ä½ç½®: {persist_directory}")
    print("=" * 50)
    
    return vectorstore


# å‘½ä»¤è¡Œå…¥å£
if __name__ == "__main__":
    ingest_pdfs()
